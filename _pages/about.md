---
permalink: /
title: "About Me"
excerpt: "About Me"
author_profile: true
classes: wide
redirect_from:
  - /about/
  - /about.html
---

I'm a Ph.D candidate at [Machine Learning and Intelligence Lab](https://mli.kaist.ac.kr/) (MLILab) in KAIST, advised by [Prof. Eunho Yang](https://scholar.google.com/citations?user=UWO1mloAAAAJ).

### Efficient Training and Inference for Foundations Models
My research focuses on enhancing the efficiency of foundation models, including large language models and mixed-modal architectures that combine text and images. I aim to improve operational efficiency by optimizing model size, minimizing computational overhead, and accelerating training and inference using techniques such as forward-only optimization, low-precision methods, and speculative decoding. The goal is to develop scalable frameworks that support the broad application of foundation models across diverse modalities.

### Generalization and Optimization in Deep Learning
My research focuses on understanding the generalization and optimization mechanisms of deep learning models, particularly through the study of loss landscapes and advancing learning algorithms grounded in theoretical insights. I have conducted extensive research on the scale-invariance of sharpness in loss landscapes, a widely recognized proxy for the generalization capability of deep learning models. Additionally, I have investigated the effectiveness of sharpness-aware minimization in overcoming local optima and achieving convergence within asymmetrical valleys. Recently, my work has expanded to apply these foundational insights to modern frameworks, including fine-tuning compressed models, zeroth-order optimization, and low-precision training for foundation models.

## Published Papers (Last Updated: Feb.3, 2025)

- <a href="https://arxiv.org/abs/2410.03355">
**LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding**
</a> \\
Doohyuk Jang<sup>†</sup>, **Sihwan Park<sup>†</sup>**, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, Eunho Yang
(<sup>†</sup>: Equal Contribution) \\
*ICLR 2025*

- <a href="https://openreview.net/pdf?id=VZ5EaTI6dqa">
**Scale-invariant Bayesian Neural Networks with Connectivity Tangent Kernel**
</a> \\
Sung-Yub Kim, **Sihwan Park**, Kyungsu Kim, Eunho Yang \\
*ICLR 2023 (Spotlighted)*

<!---
- <a href="../assets/papers/paper1.pdf">
**Scalable Task Segmentation Method Based on Change Point Detection of Multi-sensors in Smart Spaces**
</a> \\
**Sihwan Park**, Hyunju Kim, Dongman Lee \\
*Proceedings of the Korean Information Science Society Conference 2018, pp.1764-1766, Jun 2018, (Honorable Mention Award)*
-->

## Preprints
- <a href="https://openreview.net/pdf?id=Mvf5zr2qs6">
**Bias Decay Matters: Improving Large Batch Optimization with Connectivity Sharpness** 
</a> \\
Sung-Yub Kim, **Sihwan Park**, Yong-Deok Kim, Eunho Yang (2022)

- <a href="https://openreview.net/pdf?id=OBIuFjZzmp">
**MeZO-A^3dam: Memory-efficient Zeroth-order Adam with Adaptivity Adjustments for Fine-tuning LLMs**
</a> \\
**Sihwan Park<sup>†</sup>**, Jihun Yun<sup>†</sup>, Sung-Yub Kim, June Yong Yang, Yeonsung Jung, Souvik Kundu, Kyungsu Kim, Eunho Yang 
(<sup>†</sup>: Equal Contribution) (2024)

- <a href="https://arxiv.org/abs/2501.19099">
**Unraveling Zeroth-Order Optimization Through the Lens of Low-Dimensional Structured Perturbations**
</a> \\
**Sihwan Park<sup>†</sup>**, Jihun Yun<sup>†</sup>, Sung-Yub Kim, Souvik Kundu, Eunho Yang
(<sup>†</sup>: Equal Contribution) (2025)

- <a href="https://arxiv.org/abs/2502.99999">
**LANTERN++: Enhanced Relaxed Speculative Decoding with Static Tree Drafting for Visual Auto-regressive Models**
</a> \\
**Sihwan Park<sup>†</sup>**, Doohyuk Jang<sup>†</sup>, Sung-Yub Kim, Souvik Kundu, Eunho Yang
(<sup>†</sup>: Equal Contribution) (2025)
    
## Education

- **Ph.D.** in Graduate School of AI, <a href="https://gsai.kaist.ac.kr/">**Korea Advanced Institute of Science and Technology (KAIST)**</a>\\
*Sep. 2022 - Present*
  
- **M.S.** in Graduate School of AI, <a href="https://gsai.kaist.ac.kr/">**Korea Advanced Institute of Science and Technology (KAIST)**</a>\\
*Sep. 2020 - Aug. 2022*
  - Advised by [Prof. Eunho Yang](https://scholar.google.com/citations?user=UWO1mloAAAAJ)
  - Master's Thesis: <a href="../assets/papers/master_thesis.pdf">**On the Understanding of Sharpness-aware Minimization and its Application: A Perspective on Escape Efficiency and Asymmetric Valley**</a>

- **B.S.** in Computer Science, <a href="https://cs.kaist.ac.kr">**Korea Advanced Institute of Science and Technology (KAIST)**</a>\\
*Mar. 2015 - Aug. 2020*

- **B.S.** in Mathematical Science, <a href="https://mathsci.kaist.ac.kr">**Korea Advanced Institute of Science and Technology (KAIST)**</a>\\
*Mar. 2015 - Aug. 2020*

## Experiences
- Research Intern, **Computer Architecture and Systems Lab, KAIST**, Daejeon, <font size="3">Aug. 2019 - Dec. 2019</font>
  - Advisor : [Prof. Jaehyuk Huh](https://jaehyuk-huh.github.io/)
  - Low-level security techniques of Intel SGX and secure container with KVSSD

- Research Intern, **Collaborative Distributed Systems and Networking Lab, KAIST**, Daejeon, <font size="3">Jan. 2018 - Oct. 2018</font>
  - Advisor : [Prof. Dongman Lee](http://143.248.55.123/cdsn/?p=29)
  - Signal data processing for IoT task recognition and framework for task segmentation

- Exchange Student, **University of California, Santa Cruz**, Santa Cruz, CA, <font size="3">Jun. 2019 - Aug. 2019</font>
  - Software engineering and computer game basics

## Projects

- Sub-task generation based point/regional Out-Of-Distribution detection \\
**Samsung Electronics**, *Mar.2021-Sep.2025*

- Predicting graph properties with few labels using Graph Neural Networks \\
**Samsung Electronics**, *Mar.2021-Sep.2025*

- A Study on Statistically and Computationally Efficient Parameter Structures for Machine Learning Algorithms \\
**National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT)**, *Mar.2021-Dec.2022*
  
- A Study on Optimization and Network Interpretation Method for Large-Scale Machine Learning \\
**National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT)**, *Mar.2023-Feb.2027*

- A Study on Conversational Large Language Models for Virtual Physicians in Patient Intake \\
**AITRICS**, *Apr.2024-May.2024*

- Efficient Foundation Models on Intel Systems \\
**Intel Corporation & NAVER**, *Sep.2024-Aug.2027*

